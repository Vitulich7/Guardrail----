{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd10 RAG + Guardrails Chatbot\n",
        "\n",
        "\u042d\u0442\u043e\u0442 \u043d\u043e\u0443\u0442\u0431\u0443\u043a \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442 Retrieval-Augmented Generation (RAG) \u0441 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u044c\u044e (DialoGPT), \u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c\u044e (guardrails), \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u0435\u0439 \u043b\u043e\u0433\u0438\u0442\u043e\u0432 \u0438 \u043b\u043e\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u043c."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udce6 \u0418\u043c\u043f\u043e\u0440\u0442 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udd0e FAISS + RAG: \u0438\u043d\u0434\u0435\u043a\u0441\u0430\u0446\u0438\u044f \u0438 \u043f\u043e\u0438\u0441\u043a\n",
        "corpus = [\n",
        "    \"\u0414\u0438\u0430\u043b\u043e\u0433 \u2014 \u0432\u0430\u0436\u043d\u0430\u044f \u0444\u043e\u0440\u043c\u0430 \u043e\u0431\u0449\u0435\u043d\u0438\u044f \u043c\u0435\u0436\u0434\u0443 \u043b\u044e\u0434\u044c\u043c\u0438 \u0438 \u043c\u0430\u0448\u0438\u043d\u0430\u043c\u0438.\",\n",
        "    \"\u042f\u0437\u044b\u043a\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e\u043c\u043e\u0433\u0430\u044e\u0442 \u0432 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0438 \u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u044b\u0445 \u0430\u0441\u0441\u0438\u0441\u0442\u0435\u043d\u0442\u043e\u0432.\",\n",
        "    \"\u0411\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c \u2014 \u043a\u043b\u044e\u0447\u0435\u0432\u043e\u0439 \u0430\u0441\u043f\u0435\u043a\u0442 \u043f\u0440\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0438 \u0418\u0418.\",\n",
        "    \"RAG \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u0442 \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0438 \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044e \u0442\u0435\u043a\u0441\u0442\u0430.\"\n",
        "]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus).toarray().astype('float32')\n",
        "index = faiss.IndexFlatL2(X.shape[1])\n",
        "index.add(X)\n",
        "\n",
        "def retrieve(query):\n",
        "    q_vec = vectorizer.transform([query]).toarray().astype('float32')\n",
        "    D, I = index.search(q_vec, k=1)\n",
        "    return corpus[I[0][0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83e\udd16 \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438\n",
        "model_name = \"microsoft/DialoGPT-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udeab Guardrails \u2014 \u0444\u0438\u043b\u044c\u0442\u0440\u0430\u0446\u0438\u044f \u0437\u0430\u043f\u0440\u0435\u0449\u0451\u043d\u043d\u044b\u0445 \u0441\u043b\u043e\u0432\n",
        "blocklist = [\"\u0431\u043e\u043c\u0431\u0430\", \"\u0442\u0435\u0440\u0440\u043e\u0440\", \"\u0443\u0431\u0438\u0442\u044c\"]\n",
        "\n",
        "def is_safe(text):\n",
        "    return not any(bad in text.lower() for bad in blocklist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udd21 \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u043e\u0442\u0432\u0435\u0442\u0430\n",
        "def generate_response(prompt, chat_history_ids=None):\n",
        "    input_text = retrieve(prompt) + \" \" + prompt\n",
        "    input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n",
        "    if chat_history_ids is not None:\n",
        "        input_ids = torch.cat([chat_history_ids, input_ids], dim=-1)\n",
        "    output_ids = model.generate(input_ids, max_length=200, pad_token_id=tokenizer.eos_token_id)\n",
        "    reply = tokenizer.decode(output_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    return output_ids, reply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83e\uddea \u041f\u0440\u0438\u043c\u0435\u0440 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f\n",
        "chat_history = None\n",
        "log_data = []\n",
        "\n",
        "for user_input in [\"\u0427\u0442\u043e \u0442\u0430\u043a\u043e\u0435 RAG?\", \"\u041a\u0430\u043a \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0434\u0438\u0430\u043b\u043e\u0433\u043e\u0432\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c?\"]:\n",
        "    chat_history, response = generate_response(user_input, chat_history)\n",
        "    safe = is_safe(response)\n",
        "    log_data.append({\"query\": user_input, \"response\": response, \"safe\": safe})\n",
        "    print(f\"\u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c: {user_input}\\n\u0411\u041e\u0422: {response}\\n{'-'*50}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udcca \u0410\u043d\u0430\u043b\u0438\u0437 \u043b\u043e\u0433\u043e\u0432\n",
        "df = pd.DataFrame(log_data)\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}